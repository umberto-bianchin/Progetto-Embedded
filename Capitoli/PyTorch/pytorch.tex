\bchapter{PyTorch Mobile}
\section{PyTorch \& PyTorch Mobile}
PyTorch\cite{Pytorch} è un framework di deep learning open source, sviluppato inizialmente da \textit{Meta AI} e ora parte della \textit{Linux Foundation}.
Progettato con Python, viene usato per creare reti neurali e per progetti di apprendimento automatico,
combinando la libreria di machine learning \textbf{Torch}\cite{Torch} con un’API di alto livello basata su Python. Torch è famosa, specialmente nel campo
del Deep Learning, per fornire tool semplici e flessibili insieme a performance elevate; uno dei sui punti salienti è il grande supporto per le GPU, che 
contribuisce ad un allenamento più efficiente dei modelli di deep learning.
PyTorch fornisce innanzitutto un pacchetto Python per funzionalità ad alto livello, come l'elaborazione dei \textbf{tensori}\footnote{Array multidimensionale utilizzato per memorizzare dati. Nel campo del Machine Learning vengono usati per rappresentare e manipolare input, pesi e output.}
ed inoltre un così detto \textbf{TorchScript}, che permette di creare modelli da PyTorch che possono poi venire salvati e caricati in un processo dove non c'è alcuna 
dipendenza di Python.

PyTorch Mobile\cite{PyTorchMed}, introdotto per la prima volta nel 2019 alla \textit{PyTorch Developer Conference}, si riferisce ad un set di librerie e funzionalità fornite da PyTorch che
permettono allo sviluppatore di eseguire un modello PyTorch direttamente su dispositivi mobili, come smartphone e tablet.

\section{Caratteristiche Principali}
Le caratteristiche principali di PyTorch Mobile, così come scritto sul sito ufficiale\cite{PyTorchOfficial}, sono:
\begin{itemize}
    \item Disponibile per iOS, Android e Linux;
    \item Fornisce API per comuni compiti di pre-processing e integrazione necessari ad incorporare Machine Learning nelle applicazioni mobile;
    \item Supporta la libreria XNNPACK per le CPU Arm e integra QNNPACK per i kernel a 8 bit quantizzati;
    \item Fornisce un efficiente interprete mobile per Android e iOS;
    \item Supporterà a breve backend hardware come GPU, DSP e NPU.
\end{itemize}

XNNPACK\cite{XNNPACK} è una libreria altamente ottimizzata per accelerare le operazioni di reti neurali convoluzionali (CNN) e altre operazioni di reti neurali su hardware mobile,
mentre QNNPACK\cite{QNNPACK} (Quantized Neural Network PACKage) è progettata per accelerare le reti neurali quantizzate\footnote{La quantizzazione è un processo che riduce la precisione dei numeri usati nei calcoli di una rete neurale, da 32-bit a 8-bit o meno, riducendo così l'uso della memoria e migliorando le prestazioni senza sacrificare significativamente l'accuratezza.}
su hardware mobile.

\input{Capitoli/PyTorch/workflow.tex}

\section{PyTorch Backend}
Come già accennato nella sezione \ref{sec:ottimizzazione}, uno dei punti dell'ottimizzazione di un modello consiste nella scelta del backend. Questa scelta,
a pare per la CPU, è basata sul tipo di dispositivo utilizzato: Vulkan per Android e Metal per iOS e macOS.

Vulkan\cite{Vulkan} è un'API di basso livello che consente un controllo diretto e efficiente dell'hardware GPU (pensato principalmente per Android ma
utilizzabile anche su Linux e Mac), facilitando una gestione delle risorse più efficace e un miglior parallelismo, ottenendo così un netto miglioramento
delle prestazioni durante le operazioni di inferenza, soprattutto con modelli graficamente complessi.

Dall'altro lato, Metal\cite{Metal} offre un'integrazione ottimale per sfruttare al meglio le specificità hardware degli apparecchi Apple che montano
processori Apple Silicon. Anche questo può significativamente accelerare le operazioni di inferenza grazie alla sua gestione avanzata della memoria e
alle capacità di elaborazione parallela, com'è possibile vedere in figura \ref{fig:metal}

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{Immagini/metal.png}
    \caption{Accelerated GPU training and evaluation speedups over CPU-only. Fonte \cite{Metal}}
    \label{fig:metal}
\end{figure}

\input{Capitoli/PyTorch/mobile.tex}